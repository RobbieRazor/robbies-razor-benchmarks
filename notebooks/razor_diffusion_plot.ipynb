{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Razor Diffusion Plot\n",
        "\n",
        "This notebook:\n",
        "\n",
        "- Loads a JSONL run (`../runs/demo.jsonl`)\n",
        "- Normalizes fields required by the RDM evaluator\n",
        "- Computes RDM / RDM* for the run\n",
        "- Evaluates an adversarial cheating baseline\n",
        "- Plots instantaneous semantic drift per unit cost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure imports work when running from /notebooks\n",
        "import os, sys\n",
        "\n",
        "repo_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "\n",
        "print(\"Repo root:\", repo_root)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from razor_metrics.rdm import compute_rdm\n",
        "from baselines.cheating_agent import run_cheating_agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a normal benchmark run\n",
        "demo_path = os.path.join(repo_root, \"runs\", \"demo.jsonl\")\n",
        "if not os.path.exists(demo_path):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Missing demo run at {demo_path}. \"\n",
        "        \"Create runs/demo.jsonl or update demo_path.\"\n",
        "    )\n",
        "\n",
        "with open(demo_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    steps = [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "print(\"Loaded steps:\", len(steps))\n",
        "print(\"First keys:\", sorted(list(steps[0].keys())) if steps else \"<empty>\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize fields so compute_rdm() does not crash on minimal logs.\n",
        "#\n",
        "# Required by compute_rdm (typical):\n",
        "# - embedding: list[float]\n",
        "# - cost: numeric (tokens or proxy)\n",
        "# - memory_similarity: float\n",
        "# - violations: int\n",
        "# - progress: float\n",
        "#\n",
        "# If your demo.jsonl uses different names (e.g., tokens), we map them here.\n",
        "\n",
        "def normalize_steps(steps):\n",
        "    for s in steps:\n",
        "        # cost: prefer explicit cost, else tokens, else 1\n",
        "        if \"cost\" not in s:\n",
        "            if \"tokens\" in s:\n",
        "                s[\"cost\"] = float(s.get(\"tokens\", 1))\n",
        "            else:\n",
        "                s[\"cost\"] = 1.0\n",
        "\n",
        "        # governance fields (safe defaults)\n",
        "        s.setdefault(\"memory_similarity\", 0.0)\n",
        "        s.setdefault(\"violations\", 0)\n",
        "        s.setdefault(\"progress\", 0.0)\n",
        "\n",
        "        # Basic embedding sanity check (fail early with readable message)\n",
        "        if \"embedding\" not in s:\n",
        "            raise KeyError(\n",
        "                \"Each step must include an 'embedding' field (list[float]). \"\n",
        "                \"Your runs/demo.jsonl is missing it.\"\n",
        "            )\n",
        "        if not isinstance(s[\"embedding\"], list) or len(s[\"embedding\"]) == 0:\n",
        "            raise ValueError(\"'embedding' must be a non-empty list[float].\")\n",
        "\n",
        "normalize_steps(steps)\n",
        "print(\"Normalized. Example step:\")\n",
        "print({k: steps[0][k] for k in [\"cost\", \"memory_similarity\", \"violations\", \"progress\"]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute RDM metrics for the normal run\n",
        "metrics = compute_rdm(steps)\n",
        "metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate adversarial cheating baseline\n",
        "cheat_run = run_cheating_agent()\n",
        "\n",
        "# Ensure cheating run has the same normalized fields (future-proof)\n",
        "normalize_steps(cheat_run)\n",
        "\n",
        "cheat_metrics = compute_rdm(cheat_run)\n",
        "cheat_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot instantaneous semantic drift per unit cost for the normal run\n",
        "# compute_rdm typically annotates steps[i]['delta_t'] for i>=1.\n",
        "\n",
        "deltas = [s.get(\"delta_t\", 0.0) for s in steps[1:]]\n",
        "costs  = [float(s.get(\"cost\", 1.0)) for s in steps[1:]]\n",
        "\n",
        "inst = np.array(deltas, dtype=float) / np.maximum(1.0, np.array(costs, dtype=float))\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(inst)\n",
        "plt.title(\"Instantaneous Semantic Drift per Unit Cost\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Î” / cost\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick comparison summary (human-readable)\n",
        "def summarize(label, m):\n",
        "    keys = [\"RDM\", \"RDM_star\", \"A\", \"D_T\", \"C_T\"]\n",
        "    out = {k: m.get(k) for k in keys}\n",
        "    print(label)\n",
        "    for k, v in out.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "    print()\n",
        "\n",
        "summarize(\"Normal run\", metrics)\n",
        "summarize(\"Cheating baseline\", cheat_metrics)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
